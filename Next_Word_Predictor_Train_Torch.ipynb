{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Import The Libraries"
      ],
      "metadata": {
        "id": "1myX0c9NDKCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re"
      ],
      "metadata": {
        "id": "_lsinjPtDJx8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing"
      ],
      "metadata": {
        "id": "jMhHEnVlDQdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer and other parameters\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "\n",
        "# Generator to yield input-output pairs\n",
        "def text_data_generator(file_path, tokenizer, max_sequence_length):\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            line = preprocess_text(line.strip())  # Preprocess each line\n",
        "            sequence = tokenizer.texts_to_sequences([line])[0]   #Convert the sequence into digits\n",
        "            for i in range(1, len(sequence)):\n",
        "                input_sequence = sequence[:i]           #Split into input and target words\n",
        "                target_word = sequence[i]\n",
        "                # Pad the input sequence\n",
        "                input_sequence = pad_sequences([input_sequence], maxlen=max_sequence_length, padding='pre')[0]   #Pad the sequence\n",
        "                yield torch.tensor(input_sequence, dtype=torch.long), torch.tensor(target_word, dtype=torch.long)"
      ],
      "metadata": {
        "id": "p6kBWeGUDY3f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define The Model"
      ],
      "metadata": {
        "id": "4vhJmoNeEYlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Next_Word_Predictor(nn.Module):\n",
        "    def __init__(self, num_classes, embedding_dim=100, lstm_units=150,dropout_prob=0.5):\n",
        "        super(Next_Word_Predictor, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_classes, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, bidirectional=True,dropout=dropout_prob)\n",
        "        self.fc = nn.Linear(lstm_units * 2, num_classes)\n",
        "        self.dropout=nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wpOFpQd9D3Mm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the corpus and build the vocabulary"
      ],
      "metadata": {
        "id": "o-ukBocSEmHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/Corpus.txt', 'r') as file:\n",
        "    Corpus = file.read()\n",
        "\n",
        "# Tokenizer fits on the entire corpus\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([Corpus])\n",
        "vocab = tokenizer.word_index\n",
        "num_classes = len(vocab) + 1\n",
        "\n",
        "# Determine max sequence length based on corpus\n",
        "sentences = Corpus.split('\\n')\n",
        "max_sequence_length = max(len(tokenizer.texts_to_sequences([preprocess_text(sentence)])[0]) for sentence in sentences)"
      ],
      "metadata": {
        "id": "HCUVvpmzEnmV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_sequence_length)  #Need the value during testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8roqu99bV9vA",
        "outputId": "3b9ab35f-5935-42cd-a2b3-e11bbd263f2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize model, criterion, and optimizer"
      ],
      "metadata": {
        "id": "uNJ1_NSrFGNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Next_Word_Predictor(num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4Ig8DV7lEddV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21ddb03-4cd0-4dae-d312-73ae50381549"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop"
      ],
      "metadata": {
        "id": "0cpf0ZE8IYqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop using the generator\n",
        "def train_model(model, file_path, tokenizer, max_sequence_length, optimizer, num_epochs=10, steps_per_epoch=6000):\n",
        "    for epoch in range(num_epochs):\n",
        "        generator = text_data_generator(file_path, tokenizer, max_sequence_length)\n",
        "        for step in range(steps_per_epoch):\n",
        "            input_seq, target_word = next(generator)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_seq.unsqueeze(0))  # Add batch dimension\n",
        "            loss = criterion(output, target_word.unsqueeze(0))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch: {epoch + 1}/{num_epochs}\\tLoss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "q2ngX_hPITkS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, '/content/Corpus.txt', tokenizer, max_sequence_length, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4NITf1FIfR0",
        "outputId": "a82bc9c3-3860-4028-e5c6-747b2a267b0e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\tLoss: 5.2316\n",
            "Epoch: 2/10\tLoss: 4.2431\n",
            "Epoch: 3/10\tLoss: 2.4903\n",
            "Epoch: 4/10\tLoss: 1.0102\n",
            "Epoch: 5/10\tLoss: 0.9458\n",
            "Epoch: 6/10\tLoss: 0.0690\n",
            "Epoch: 7/10\tLoss: 0.6529\n",
            "Epoch: 8/10\tLoss: 0.1621\n",
            "Epoch: 9/10\tLoss: 0.1580\n",
            "Epoch: 10/10\tLoss: 0.2695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Save The Model And The Vocabulary"
      ],
      "metadata": {
        "id": "G5QlUeCkQGv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'/content/next_word_predictor.pth')"
      ],
      "metadata": {
        "id": "rJazsuIGQJwa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Save the vocabulary\n",
        "with open('/content/vocabulary.json', 'w') as f:\n",
        "    json.dump(tokenizer.word_index, f)"
      ],
      "metadata": {
        "id": "vVnDIrOyS3wW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sample Code To Generate A Corpus"
      ],
      "metadata": {
        "id": "LJATGzbRMfXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import random\n",
        "#\n",
        "## Example templates for sentence structures\n",
        "#templates = [\n",
        "#    \"The {} {} {} the {}.\",  # Needs 4 arguments\n",
        "#    \"{} {} {} to the {}.\",  # Needs 4 arguments\n",
        "#    \"{} {} {} a {} {}.\",    # Needs 5 arguments\n",
        "#    \"In the {}, the {} {} {}.\",  # Needs 4 arguments\n",
        "#    \"After the {}, the {} {} {} {}.\"  # Needs 5 arguments\n",
        "#]\n",
        "#\n",
        "## Example vocabularies\n",
        "#nouns = [\"cat\", \"dog\", \"car\", \"man\", \"woman\", \"child\", \"building\", \"tree\", \"river\", \"mountain\"]\n",
        "#verbs = [\"jumps\", \"runs\", \"drives\", \"flies\", \"walks\", \"sings\", \"dances\", \"writes\", \"reads\", \"talks\"]\n",
        "#adjectives = [\"quick\", \"lazy\", \"tall\", \"short\", \"bright\", \"dark\", \"happy\", \"sad\", \"big\", \"small\"]\n",
        "#places = [\"park\", \"city\", \"forest\", \"beach\", \"school\", \"office\", \"home\", \"village\", \"market\", \"station\"]\n",
        "#\n",
        "## Generate sentences\n",
        "#corpus = []\n",
        "#for _ in range(3000):  # Generate 1000 sentences\n",
        "#    template = random.choice(templates)\n",
        "#    if template.count('{}') == 4:\n",
        "#        # Choose 4 arguments if the template requires 4 placeholders\n",
        "#        sentence = template.format(\n",
        "#            random.choice(adjectives),\n",
        "#            random.choice(nouns),\n",
        "#            random.choice(verbs),\n",
        "#            random.choice(places)\n",
        "#        )\n",
        "#    else:\n",
        "#        # Choose 5 arguments if the template requires 5 placeholders\n",
        "#        sentence = template.format(\n",
        "#            random.choice(adjectives),\n",
        "#            random.choice(nouns),\n",
        "#            random.choice(verbs),\n",
        "#            random.choice(adjectives),\n",
        "#            random.choice(places)\n",
        "#        )\n",
        "#    corpus.append(sentence)\n",
        "#\n",
        "## Join sentences into a corpus\n",
        "#generated_corpus = \"\\n\".join(corpus)\n",
        "#\n",
        "## Save the corpus to a file\n",
        "#output_file = '/content/generated_corpus.txt'\n",
        "#with open(output_file, 'w') as file:\n",
        "#    file.write(generated_corpus)\n",
        "#\n",
        "#print(f\"Corpus saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "mhJ8BnLGL6hF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}